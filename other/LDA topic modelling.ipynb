{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after downloading the json file from le grand debat i transformed it to data_file.csv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "\n",
    "f = \"data_file.csv\"\n",
    "df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['reference', 'title', 'createdAt', 'publishedAt', 'updatedAt',\n",
       "       'trashed', 'trashedStatus', 'authorId', 'authorType', 'authorZipCode',\n",
       "       'responses'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing to extract the wanted answers list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "responces = df.responses\n",
    "answers=[]\n",
    "for responce in responces:\n",
    "    r=responce.split(\"questionId\")[-1].split(\"formattedValue\")[-1]\n",
    "    responce = r[4:len(r)-3]\n",
    "    responce=responce.replace(\"\\\\n\",\"\")\n",
    "    answers.append(responce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok=True\n",
    "while(ok):\n",
    "    try:\n",
    "        answers.remove(\"on\")\n",
    "    except : \n",
    "        ok=False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pd.Series(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                   Multiplier les centrales géothermiques\n",
      "1        Les problèmes auxquels se trouve confronté l’e...\n",
      "2          Une vrai politique écologique et non économique\n",
      "3        Les bonnes idées ne grandissent que par le par...\n",
      "4        Pédagogie dans ce sens là dés la petite école ...\n",
      "                               ...                        \n",
      "33102    Construire des usines marée motrice, des panne...\n",
      "33103                                                  Non\n",
      "33104    Demandons aux grandes chaines de fast food de ...\n",
      "33105    REMUEZ-VOUS, ARRETEZ VOTRE BLABLA .... DES ACTES,\n",
      "33106    L'Etat devrait davantage communiquer sur les a...\n",
      "Length: 33107, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 40s\n"
     ]
    }
   ],
   "source": [
    "%time spacy_docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we remove all words shorter than 3 characters (these are often fairly uninteresting from a topical point of view) <br>\n",
    "we drop all stopwords <br>\n",
    "we take them lemmas of the remaining words and lowercase them <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [[t.lemma_.lower() for t in doc if len(t.orth_) > 3 and not t.is_stop] for doc in spacy_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['multiplier', 'centrale', 'géothermique'], ['problème', 'trouve', 'confronter', 'ensemble', 'planète', 'dénoncent', 'parfaire', 'désordre', 'gilet', 'jaune', 'france', 'il', 'surpopulation', 'mondial', 'population', 'passer', 'd’1,5', 'milliard', 'habitant', '1900', 'milliard', '2020', 'monter', 'bientôt', 'milliard', '2040', 'progrès', 'communication', 'village', 'mondial', 'individu', 'fond', 'asie', 'fond', 'afrique', 'passer', 'quartiers', 'campagne', 'pays', 'aspir', 'vivre', 'blâmer', 'lotir', 'concitoyen', 'logement', 'nourriture', 'bien', 'consommation', 'déplacement', 'etc.', 'mère', 'problème', 'bien', 'solution', 'problème', 'stabilisation', 'croissance', 'démographique', 'partage', 'richesse', 'partage', 'terre', 'partage', 'protection', 'biodiversité', 'règlemer', 'conflit', 'lutte', 'contre', 'déforestation', 'lutte', 'contre', 'dérèglemer', 'climatique', 'règlemer', 'conflit', 'stabilisation', 'migration', 'concurrence', 'commercial', 'mondial', 'etc.', 'français', 'européen', 'mondial', 'france', 'jouer', 'rôle', 'moteur', 'autour', 'dérouler', 'grand', 'débat', 'paraître', 'anecdotique'], ['vrai', 'politique', 'écologique', 'économique']]\n"
     ]
    }
   ],
   "source": [
    "print(docs[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rq : topic models are bag-of-word models that ignore word position <br>\n",
    "we use the Gensim library to  identify the frequent bigrams in the corpus, then we append them to the list of tokens for the documents in which they appear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vrai', 'politique', 'écologique', 'économique']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from gensim.models import Phrases\n",
    "\n",
    "bigram = Phrases(docs, min_count=10)\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:  # bigrams can be recognized by the \"_\" that joins the invidual words\n",
    "            docs[idx].append(token)\n",
    "docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in original documents: 53657\n",
      "Number of unique words after removing rare and common words: 16080\n",
      "Example representation of document 3: [(87, 1), (88, 1), (89, 1), (90, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(docs)\n",
    "print('Number of unique words in original documents:', len(dictionary))\n",
    "\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.25)\n",
    "print('Number of unique words after removing rare and common words:', len(dictionary))\n",
    "\n",
    "print(\"Example representation of document 3:\", dictionary.doc2bow(docs[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bag-of-word representations for each document in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus: the bag-of-word representations of our documents <br>\n",
    "id2token: the mapping from indices to words <br>\n",
    "num_topics: the number of topics we want the model to identify  <br>\n",
    "chunksize: the number of documents the model sees for every update        equivalent to batch size<br>\n",
    "passes: the number of times we show the total corpus to the model during training  <br>\n",
    "random_state: we use a seed to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46.9 s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "%time model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, chunksize=1000, passes=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 0.036*\"nucléaire\" + 0.027*\"énergie\" + 0.015*\"falloir\" + 0.015*\"production\" + 0.015*\"électricité\" + 0.015*\"centrale\" + 0.015*\"centrale_nucléaire\" + 0.013*\"solution\" + 0.011*\"éolien\" + 0.010*\"france\"\n",
      "2 : 0.019*\"panneau_solaire\" + 0.016*\"lutter_contre\" + 0.016*\"ville\" + 0.015*\"solaire\" + 0.014*\"panneau\" + 0.012*\"centre_ville\" + 0.010*\"grand_surface\" + 0.010*\"grand\" + 0.009*\"construction\" + 0.008*\"bâtiment\"\n",
      "3 : 0.046*\"mettre_place\" + 0.045*\"énergie_renouvelable\" + 0.035*\"énergie\" + 0.019*\"place\" + 0.018*\"énergie_fossile\" + 0.018*\"mettre\" + 0.018*\"transition_énergétique\" + 0.017*\"renouvelable\" + 0.014*\"énergétique\" + 0.007*\"service_civique\"\n",
      "4 : 0.011*\"falloir\" + 0.011*\"faire\" + 0.009*\"bien\" + 0.007*\"politique\" + 0.006*\"pouvoir\" + 0.006*\"réchauffement_climatique\" + 0.006*\"environnement\" + 0.006*\"devoir\" + 0.005*\"problème\" + 0.005*\"citoyen\"\n",
      "5 : 0.043*\"produit\" + 0.017*\"taxer\" + 0.011*\"production\" + 0.011*\"sac_plastique\" + 0.010*\"obsolescence_programmer\" + 0.009*\"produit_importer\" + 0.009*\"transport\" + 0.009*\"taxe\" + 0.009*\"entreprise\" + 0.008*\"pollueur\"\n",
      "6 : 0.083*\"transition_écologique\" + 0.042*\"transition\" + 0.041*\"écologique\" + 0.017*\"faire\" + 0.011*\"falloir\" + 0.009*\"citoyen\" + 0.009*\"taxe\" + 0.009*\"grand\" + 0.008*\"pouvoir\" + 0.008*\"entreprise\"\n",
      "7 : 0.030*\"déchet\" + 0.019*\"faire_payer\" + 0.018*\"faire\" + 0.016*\"long_terme\" + 0.011*\"payer\" + 0.011*\"taxe_carbon\" + 0.009*\"traitement_déchet\" + 0.009*\"volonté_politique\" + 0.008*\"recyclage_déchet\" + 0.008*\"recyclage\"\n",
      "8 : 0.024*\"produit\" + 0.019*\"agriculture\" + 0.018*\"interdir\" + 0.013*\"plastique\" + 0.013*\"animal\" + 0.012*\"pesticide\" + 0.011*\"agriculteur\" + 0.009*\"glyphosate\" + 0.008*\"chasse\" + 0.008*\"agriculture_biologique\"\n",
      "9 : 0.024*\"voiture_électrique\" + 0.024*\"électrique\" + 0.023*\"voiture\" + 0.018*\"véhicule\" + 0.016*\"véhicule_électrique\" + 0.012*\"diesel\" + 0.010*\"batterie\" + 0.009*\"falloir\" + 0.008*\"faire\" + 0.008*\"c\\'est\"\n",
      "10 : 0.023*\"transport\" + 0.016*\"transport_commun\" + 0.009*\"taxer\" + 0.007*\"mise_place\" + 0.007*\"avion\" + 0.007*\"taxe\" + 0.007*\"travail\" + 0.007*\"voiture\" + 0.007*\"véhicule\" + 0.006*\"commun\"\n"
     ]
    }
   ],
   "source": [
    "for (topic, words) in model.print_topics():\n",
    "    print(topic+1, \":\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
