{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"characterGenData.txt\",encoding = \"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\newgh\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\newgh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\newgh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\newgh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\newgh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\newgh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\newgh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\newgh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\newgh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\newgh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import sys\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(input):\n",
    "    input = input.lower()\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_inputs = tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 107450\n",
      "Total vocab: 42\n"
     ]
    }
   ],
   "source": [
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print (\"Total number of characters:\", input_len)\n",
    "print (\"Total vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input will be a sequence of 100 characters and output with be 1 char that comes after those 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, input_len - seq_length, 1):\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_patterns = len(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshape + chars are represented as 0-->47 we transform them to 0 --> 47/48 so they can be interpreted by the sigmoid func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np_utils.to_categorical(y_data)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 2.5494\n",
      "Epoch 00001: loss improved from 2.63295 to 2.54943, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 96s 219ms/step - loss: 2.5494\n",
      "Epoch 2/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 2.4849\n",
      "Epoch 00002: loss improved from 2.54943 to 2.48489, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 92s 219ms/step - loss: 2.4849\n",
      "Epoch 3/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 2.4208\n",
      "Epoch 00003: loss improved from 2.48489 to 2.42076, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 216ms/step - loss: 2.4208\n",
      "Epoch 4/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 2.3637\n",
      "Epoch 00004: loss improved from 2.42076 to 2.36371, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 216ms/step - loss: 2.3637\n",
      "Epoch 5/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 2.3092\n",
      "Epoch 00005: loss improved from 2.36371 to 2.30924, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 217ms/step - loss: 2.3092\n",
      "Epoch 6/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 2.2632\n",
      "Epoch 00006: loss improved from 2.30924 to 2.26320, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 217ms/step - loss: 2.2632\n",
      "Epoch 7/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 2.2180\n",
      "Epoch 00007: loss improved from 2.26320 to 2.21796, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 217ms/step - loss: 2.2180\n",
      "Epoch 8/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 2.1774\n",
      "Epoch 00008: loss improved from 2.21796 to 2.17740, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 217ms/step - loss: 2.1774\n",
      "Epoch 9/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 2.1378\n",
      "Epoch 00009: loss improved from 2.17740 to 2.13780, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 216ms/step - loss: 2.1378\n",
      "Epoch 10/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 2.1018\n",
      "Epoch 00010: loss improved from 2.13780 to 2.10176, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 216ms/step - loss: 2.1018\n",
      "Epoch 11/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 2.0724\n",
      "Epoch 00011: loss improved from 2.10176 to 2.07237, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 216ms/step - loss: 2.0724\n",
      "Epoch 12/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 2.0440\n",
      "Epoch 00012: loss improved from 2.07237 to 2.04395, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 216ms/step - loss: 2.0440\n",
      "Epoch 13/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 2.0128\n",
      "Epoch 00013: loss improved from 2.04395 to 2.01278, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 217ms/step - loss: 2.0128\n",
      "Epoch 14/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 1.9912\n",
      "Epoch 00014: loss improved from 2.01278 to 1.99117, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 216ms/step - loss: 1.9912\n",
      "Epoch 15/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 1.9700\n",
      "Epoch 00015: loss improved from 1.99117 to 1.97002, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 216ms/step - loss: 1.9700\n",
      "Epoch 16/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 1.9453\n",
      "Epoch 00016: loss improved from 1.97002 to 1.94526, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 216ms/step - loss: 1.9453\n",
      "Epoch 17/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 1.9256\n",
      "Epoch 00017: loss improved from 1.94526 to 1.92563, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 216ms/step - loss: 1.9256\n",
      "Epoch 18/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 1.9087\n",
      "Epoch 00018: loss improved from 1.92563 to 1.90873, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 216ms/step - loss: 1.9087\n",
      "Epoch 19/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 1.8891\n",
      "Epoch 00019: loss improved from 1.90873 to 1.88908, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 217ms/step - loss: 1.8891\n",
      "Epoch 20/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 1.8699\n",
      "Epoch 00020: loss improved from 1.88908 to 1.86988, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 217ms/step - loss: 1.8699\n",
      "Epoch 21/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 1.8542\n",
      "Epoch 00021: loss improved from 1.86988 to 1.85422, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 217ms/step - loss: 1.8542\n",
      "Epoch 22/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 1.8417\n",
      "Epoch 00022: loss improved from 1.85422 to 1.84167, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 217ms/step - loss: 1.8417\n",
      "Epoch 23/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 1.8311\n",
      "Epoch 00023: loss improved from 1.84167 to 1.83113, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 217ms/step - loss: 1.8311\n",
      "Epoch 24/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 1.8183\n",
      "Epoch 00024: loss improved from 1.83113 to 1.81829, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 217ms/step - loss: 1.8183\n",
      "Epoch 25/25\n",
      "420/420 [==============================] - ETA: 0s - loss: 1.8069\n",
      "Epoch 00025: loss improved from 1.81829 to 1.80689, saving model to model_weights_saved.hdf5\n",
      "420/420 [==============================] - 91s 217ms/step - loss: 1.8069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c7638b8b08>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=25, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "picking a random pattern from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "\" d found father inexorable quitted country returned heard former mistress married according inclinati \"\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, n_patterns - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed:\")\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on sear sense sear sense sear sense sear sense sear sense sear sense sear sense sear sense sear sense sear sense sear sense sear sense sear sense sear sense sear sense sear sense sear sense sear sense"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    #prepare the input\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_len)\n",
    "    \n",
    "    #pridict the output letter\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    #pick the most probable letter\n",
    "    index = numpy.argmax(prediction)\n",
    "    #get the char and print it\n",
    "    result = num_to_char[index]\n",
    "    sys.stdout.write(result)\n",
    "    \n",
    "    #append the index and move the pattern to the next squence\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
